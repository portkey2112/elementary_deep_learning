{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_from_scratch_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPD9c7gp3VDAssPuGojyriv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/portkey2112/elementary_deep_learning/blob/nn_using_numpy/NN_from_scratch_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5H6MVA5Tzpf"
      },
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxKxtzd3UTWE"
      },
      "source": [
        "X, Y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Fej--6UfsZ"
      },
      "source": [
        "#X.shape\n",
        "#X[0]\n",
        "\n",
        "X = X.astype(np.float32)\n",
        "X /= 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bILx8IAgUoUq",
        "outputId": "dbc9c6ff-ce23-4958-daac-f08b5d5dedbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#y.shape\n",
        "Y = Y.astype(np.int32)\n",
        "Y = Y.reshape(Y.shape[0], 1)\n",
        "Y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_EDZf1mUpF9",
        "outputId": "070bde81-8cfb-4733-c1e2-5f418b427e23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.imshow(X[1].reshape(28, 28), cmap='gray')\n",
        "Y[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p32tmsnUq9-",
        "outputId": "9decd35c-70cc-4b24-c8bf-d6d5bf95dac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#datasets %: 60: 20: 20\n",
        "#looks like the dataset is already shuffled.\n",
        "\n",
        "X = X.T\n",
        "Y = Y.T\n",
        "\n",
        "total = Y.shape[1]\n",
        "m = (60/100) * total\n",
        "m_dev = (20/100) * total\n",
        "m_test = (20/100) * total\n",
        "\n",
        "print(m, m_dev, m_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42000.0 14000.0 14000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vekPe0mnUuUM"
      },
      "source": [
        "X_train = X[:, 0:42000]\n",
        "Y_train = Y[:, 0:42000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q6g5paFUwff"
      },
      "source": [
        "X_dev = X[:, 42000:56000]\n",
        "Y_dev = Y[:, 42000:56000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9s9Vc-LUyCJ"
      },
      "source": [
        "X_test = X[:, 56000:]\n",
        "Y_test = Y[:, 56000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzyx5vSUU1Dc"
      },
      "source": [
        "X_test = X[:, 56000:]\n",
        "Y_test = Y[:, 56000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUOR0oyuU2rx",
        "outputId": "1ab59fed-0c36-4b75-f3e8-c471615e544e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "print('X_train.shape ', X_train.shape)\n",
        "print('Y_train.shape ', Y_train.shape)\n",
        "\n",
        "print('X_dev.shape ', X_dev.shape)\n",
        "print('Y_dev.shape ', Y_dev.shape)\n",
        "\n",
        "print('X_test.shape ', X_test.shape)\n",
        "print('Y_test.shape ', Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape  (784, 42000)\n",
            "Y_train.shape  (1, 42000)\n",
            "X_dev.shape  (784, 14000)\n",
            "Y_dev.shape  (1, 14000)\n",
            "X_test.shape  (784, 14000)\n",
            "Y_test.shape  (1, 14000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfpT6UmsU4Lj"
      },
      "source": [
        "layers = [784, 1568, 1568, 10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yOYsZOdU6Ah",
        "outputId": "1c24d4ce-dc67-4921-b568-84f921048152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 1/ (1 + np.exp(-z))\n",
        "\n",
        "sigmoid(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6YnSP_nU7y7",
        "outputId": "5d8f7b30-0218-4303-d3f6-4c1035cb5a85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def relu(z):\n",
        "    return np.array([i if(i >= 0) else 0 for i in z])\n",
        "\n",
        "relu(np.array([1, 2, -1 ,0, 2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0, 0, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3hnykH9U9oh",
        "outputId": "a62429c2-b1a1-4592-c29a-6abb2f598c42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def init(layers):\n",
        "    num_layers = len(layers)\n",
        "    params = {}\n",
        "    \n",
        "    for i in range(1, num_layers):\n",
        "        # init W to control vanishing/ exploding gradients problem\n",
        "        params['W'+str(i-1)] = np.random.randn(layers[i], layers[i-1]) * np.sqrt(1/layers[i])  \n",
        "        params['b'+str(i-1)] = np.zeros([layers[i], 1])\n",
        "        print(params['W'+str(i-1)].shape)\n",
        "        print(params['b'+str(i-1)].shape)\n",
        "    \n",
        "    return params\n",
        "\n",
        "params = init(layers)\n",
        "print(params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1568, 784)\n",
            "(1568, 1)\n",
            "(1568, 1568)\n",
            "(1568, 1)\n",
            "(10, 1568)\n",
            "(10, 1)\n",
            "{'W0': array([[-0.03916561,  0.01111939,  0.02191535, ...,  0.00914804,\n",
            "         0.01026842,  0.01301973],\n",
            "       [ 0.01049816,  0.05065812, -0.0420527 , ..., -0.00963657,\n",
            "        -0.03579707,  0.0007283 ],\n",
            "       [-0.02805569,  0.03240664, -0.03462926, ...,  0.01511536,\n",
            "        -0.0203059 , -0.01719519],\n",
            "       ...,\n",
            "       [-0.01347002, -0.02574748, -0.00053602, ..., -0.01023363,\n",
            "         0.01648383, -0.03472547],\n",
            "       [-0.00851384, -0.00225596, -0.01565225, ...,  0.00257248,\n",
            "         0.02016894, -0.01023506],\n",
            "       [-0.04432075, -0.03047809, -0.01922702, ...,  0.04903839,\n",
            "         0.03731996, -0.02814069]]), 'b0': array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       ...,\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]]), 'W1': array([[ 0.0208112 , -0.00336528,  0.0196669 , ..., -0.00685239,\n",
            "         0.05682916, -0.06676359],\n",
            "       [ 0.00942678, -0.03282742,  0.00468103, ..., -0.0021167 ,\n",
            "         0.00225711, -0.01456269],\n",
            "       [ 0.02260869,  0.02394436,  0.00168177, ..., -0.00414669,\n",
            "        -0.0135212 ,  0.01986269],\n",
            "       ...,\n",
            "       [-0.01061738,  0.0052746 ,  0.00853125, ...,  0.00331057,\n",
            "        -0.02699853,  0.05279747],\n",
            "       [-0.00938764, -0.00016873, -0.01296433, ..., -0.02703063,\n",
            "        -0.04569878,  0.0174612 ],\n",
            "       [ 0.01090751, -0.03396761, -0.01221973, ...,  0.02555764,\n",
            "        -0.04127364, -0.02262256]]), 'b1': array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       ...,\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]]), 'W2': array([[-0.00506197, -0.14721366, -0.16811431, ..., -0.00921585,\n",
            "        -0.52624508, -0.00109501],\n",
            "       [-0.07403202,  0.27267943,  0.54043202, ..., -0.20755919,\n",
            "         0.04544052,  0.3238671 ],\n",
            "       [ 0.32894947,  0.25528919, -0.20055393, ..., -0.13660436,\n",
            "         1.08057543, -0.01128907],\n",
            "       ...,\n",
            "       [ 0.26165998,  0.06568125,  0.09400465, ..., -0.35017537,\n",
            "        -0.15306801, -0.08002414],\n",
            "       [ 0.03589951, -0.2294245 ,  0.05286377, ..., -0.24940945,\n",
            "         0.361572  ,  0.10499237],\n",
            "       [ 0.15322202,  0.05360741, -0.27125632, ..., -0.12156968,\n",
            "         0.08549179, -0.72325286]]), 'b2': array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CONcmsHSU_gD"
      },
      "source": [
        "def softmax(Z):\n",
        "    '''\n",
        "    Z = np.exp(Z)\n",
        "    m = Z.shape[1]\n",
        "    for i in range(m):\n",
        "        temp = np.sum(Z[:, i])\n",
        "        Z[:, i] = Z[:, i]/ temp\n",
        "        #print(Z[:, i])\n",
        "    \n",
        "    return Z \n",
        "    '''\n",
        "    #print(np.sum(np.exp(Z), axis=0))\n",
        "    #print(\"axis = 0\", np.sum(np.exp(Z)))\n",
        "    return np.exp(Z)/np.sum(np.exp(Z), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGiZaI_iVChh",
        "outputId": "faa1bb61-c72f-470f-f6f6-fbb487a68ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "def one_hot_encode(Y, num_classes):\n",
        "    m = Y.shape[1]\n",
        "    print(type(Y[0][0]))\n",
        "    Y_train = np.zeros((num_classes, Y.shape[1]))\n",
        "    Y_train[Y, np.arange(m)] = 1\n",
        "    print(np.arange(m).shape)\n",
        "    print(Y.shape)\n",
        "    return Y_train\n",
        "\n",
        "\n",
        "Y_train_oneHot = one_hot_encode(Y_train, 10)\n",
        "print(Y_train)\n",
        "print(Y_train_oneHot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.int32'>\n",
            "(42000,)\n",
            "(1, 42000)\n",
            "[[5 0 4 ... 2 7 0]]\n",
            "[[0. 1. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydr0vMybVEbS"
      },
      "source": [
        "def forward_prop(X_train, Y_train, params, layers):\n",
        "    n = X_train.shape[1]\n",
        "    num_layers = len(layers)\n",
        "    cache = {}\n",
        "    cache['A0'] = X_train\n",
        "    \n",
        "    for i in range(1, num_layers):\n",
        "        cache['Z'+str(i)] = np.dot(params['W' + str(i-1)], cache['A'+str(i-1)]) + params['b'+str(i-1)]\n",
        "        if(i != num_layers-1):\n",
        "            cache['A'+str(i)] = np.tanh(cache['Z'+str(i)])\n",
        "        else:\n",
        "            cache['A'+str(i)] = softmax(cache['Z'+str(i)])\n",
        "        \n",
        "        assert(cache['Z'+str(i)].shape[0] == params['W' + str(i-1)].shape[0] and cache['Z'+str(i)].shape[1] == n)\n",
        "\n",
        "    return cache\n",
        "\n",
        "cache = forward_prop(X_train, Y_train_oneHot, params, layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P8AoPvkVG8g",
        "outputId": "70f1e2d8-7ce9-4710-a04b-84b1d0a968d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "def cost(Y, h):\n",
        "    m = Y.shape[1]\n",
        "    return -(1/m) * np.sum(np.sum(np.multiply(Y, np.log(h))))\n",
        "\n",
        "\n",
        "print(\"cost = \", cost(Y_train_oneHot, cache['A3']))\n",
        "print(cache['A3'])\n",
        "print(cache['A3'].shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_train_oneHot.shape)\n",
        "print(cache['A3'][:,0])\n",
        "print(np.sum(cache['A3'][:,0]))\n",
        "print(np.sum(cache['A3'][:,1]))\n",
        "print(np.sum(cache['A3'][:,2]))\n",
        "print(np.sum(cache['A3'][:,3]))\n",
        "cache['A3'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cost =  3.6970026865897507\n",
            "[[1.12419536e-03 3.18486280e-02 3.83470191e-04 ... 3.05665212e-03\n",
            "  1.94987119e-03 8.58459553e-04]\n",
            " [9.72033045e-02 2.00600290e-01 1.79534467e-03 ... 5.44199216e-01\n",
            "  7.51519429e-01 5.69174856e-01]\n",
            " [4.33340150e-04 4.92592288e-02 8.75860845e-01 ... 1.12232544e-03\n",
            "  2.14793998e-03 1.19822221e-02]\n",
            " ...\n",
            " [1.01101564e-02 2.26387365e-02 1.27478503e-03 ... 5.33824939e-03\n",
            "  3.27730590e-02 1.40115541e-02]\n",
            " [2.93965235e-04 4.15163171e-03 1.85253805e-03 ... 7.26295090e-06\n",
            "  1.61284532e-04 2.38696579e-03]\n",
            " [3.87660152e-02 2.04235830e-01 3.07746629e-02 ... 3.07143956e-04\n",
            "  1.47005397e-02 3.87462990e-02]]\n",
            "(10, 42000)\n",
            "(1, 42000)\n",
            "(10, 42000)\n",
            "[1.12419536e-03 9.72033045e-02 4.33340150e-04 7.35336424e-01\n",
            " 1.08261215e-01 1.28189370e-03 7.18948989e-03 1.01101564e-02\n",
            " 2.93965235e-04 3.87660152e-02]\n",
            "1.0\n",
            "0.9999999999999998\n",
            "0.9999999999999999\n",
            "0.9999999999999999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 42000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RUMBPDIVQov",
        "outputId": "ea90c558-8541-46f1-fe80-8fed22331414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#https://peterroelants.github.io/posts/cross-entropy-softmax/\n",
        "def backprop(X_train, Y_train, params, cache):\n",
        "    \n",
        "    m = X_train.shape[1]\n",
        "    \n",
        "    print(Y_train.shape)\n",
        "    print(cache['A3'].shape)\n",
        "    \n",
        "    #important to multiply with 1/m. we are taking average in the next steps\n",
        "    #important: its (A3 - Y) and not (Y - A3)! Use the latter and see the cost INCREASING in every iter! So, DONT.\n",
        "    dZ3 = (1/m)*(cache['A3'] - Y_train)    \n",
        "    assert(dZ3.shape == cache['Z3'].shape)\n",
        "    \n",
        "    dW2 = np.dot(dZ3, cache['A2'].T)\n",
        "    assert(dW2.shape == params['W2'].shape)\n",
        "    \n",
        "    db2 = np.sum(dZ3, axis=1, keepdims=True)\n",
        "    assert(db2.shape == params['b2'].shape)\n",
        "    \n",
        "    dA2 = np.dot(params['W2'].T, dZ3)\n",
        "    assert(dA2.shape == cache['A2'].shape)\n",
        "    \n",
        "    dZ2 = np.multiply(dA2, 1 - np.power(cache['A2'], 2))\n",
        "    assert(dZ2.shape == cache['Z2'].shape)\n",
        "    \n",
        "    dW1 = np.dot(dZ2, cache['A1'].T)\n",
        "    assert(dW1.shape == params['W1'].shape)\n",
        "    \n",
        "    db1 = np.sum(dZ2, axis=1, keepdims=True)\n",
        "    assert(db1.shape == params['b1'].shape)\n",
        "    \n",
        "    dA1 = np.dot(params['W1'].T, dZ2)\n",
        "    assert(dA1.shape == cache['A1'].shape)\n",
        "    \n",
        "    dZ1 = np.multiply(dA1, 1 - np.power(cache['A1'], 2))\n",
        "    assert(dZ1.shape == cache['Z1'].shape)\n",
        "    \n",
        "    dW0 = np.dot(dZ1, cache['A0'].T)\n",
        "    assert(dW0.shape == params['W0'].shape) \n",
        "    \n",
        "    db0 = np.sum(dZ1, axis=1, keepdims=True)\n",
        "    assert(db0.shape == params['b0'].shape)\n",
        "    \n",
        "    grads = {'dW0': dW0, 'db0': db0, 'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
        "    return grads\n",
        "    \n",
        "grads = backprop(X_train, Y_train_oneHot, params, cache)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 42000)\n",
            "(10, 42000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH_KRNFmVVm1"
      },
      "source": [
        "def update_params(params, grads, learning_rate=0.001):\n",
        "    params['W0'] = params['W0'] - learning_rate * grads['dW0']\n",
        "    params['b0'] = params['b0'] - learning_rate * grads['db0']\n",
        "    params['W1'] = params['W1'] - learning_rate * grads['dW1']\n",
        "    params['b1'] = params['b1'] - learning_rate * grads['db1']\n",
        "    params['W2'] = params['W2'] - learning_rate * grads['dW2']\n",
        "    params['b2'] = params['b2'] - learning_rate * grads['db2']\n",
        "    \n",
        "    return params\n",
        "\n",
        "params = update_params(params, grads)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R29YDFCaVcvk",
        "outputId": "5fcde111-ec65-48a9-ccf3-2d30c7c3d288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "def unroll(params):\n",
        "    #theta = np.zeros((-1, 1))\n",
        "    \n",
        "    for ind in ['W0', 'b0', 'W1', 'b1', 'W2', 'b2']:\n",
        "        print(params[ind].shape,\"+++++\")\n",
        "        print(params[ind].reshape(-1, 1).shape,\"______\")\n",
        "        p = params[ind].reshape(-1, 1)\n",
        "        if(ind == 'W0'):\n",
        "            theta = p\n",
        "        else:\n",
        "            theta = np.concatenate((theta, p), axis = 0)\n",
        "        print(p.shape)\n",
        "        print(theta.shape)\n",
        "        #print(theta)\n",
        "        \n",
        "    return theta\n",
        "\n",
        "unroll_params = unroll(params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1568, 784) +++++\n",
            "(1229312, 1) ______\n",
            "(1229312, 1)\n",
            "(1229312, 1)\n",
            "(1568, 1) +++++\n",
            "(1568, 1) ______\n",
            "(1568, 1)\n",
            "(1230880, 1)\n",
            "(1568, 1568) +++++\n",
            "(2458624, 1) ______\n",
            "(2458624, 1)\n",
            "(3689504, 1)\n",
            "(1568, 1) +++++\n",
            "(1568, 1) ______\n",
            "(1568, 1)\n",
            "(3691072, 1)\n",
            "(10, 1568) +++++\n",
            "(15680, 1) ______\n",
            "(15680, 1)\n",
            "(3706752, 1)\n",
            "(10, 1) +++++\n",
            "(10, 1) ______\n",
            "(10, 1)\n",
            "(3706762, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ApHaZ_wVfAU"
      },
      "source": [
        "#layers = [784, 1568, 1568, 10]\n",
        "def roll(theta):\n",
        "    sh1 = (1568, 784)   #shape of w1\n",
        "    ele1 = 1568 * 784   # no. of elements in w1\n",
        "    sh2 = (1568, 1568)\n",
        "    ele2 = 1568 * 1568\n",
        "    sh3 = (10, 1568)\n",
        "    ele3 = 10 * 1568\n",
        "    \n",
        "    l1 = 784     #no. of units in layer 1\n",
        "    l2 = 1568\n",
        "    l3 = 1568\n",
        "    l4 = 10\n",
        "    \n",
        "    params = {}\n",
        "    \n",
        "    params['W0'] = theta[0:ele1, :].reshape(sh1)\n",
        "    params['b0'] = theta[ele1:ele1+l2, :].reshape(l2, 1)\n",
        "    assert(params['W0'].shape == sh1)\n",
        "    assert(params['b0'].shape == (l2, 1))\n",
        "    \n",
        "    params['W1'] = theta[ele1+l2 : ele1+l2+ele2, :].reshape(sh2)\n",
        "    params['b1'] = theta[ele1+l2+ele2 : ele1+l2+ele2+l3, :].reshape(l3, 1)\n",
        "    assert(params['W1'].shape == sh2)\n",
        "    assert(params['b1'].shape == (l3, 1))\n",
        "    \n",
        "    params['W2'] = theta[ele1+l2+ele2+l3: ele1+l2+ele2+l3+ele3, : ].reshape(sh3)\n",
        "    params['b2'] = theta[ele1+l2+ele2+l3+ele3: , :].reshape(l4, 1)\n",
        "    assert(params['W2'].shape == sh3)\n",
        "    assert(params['b2'].shape == (l4, 1))\n",
        "    \n",
        "    return params\n",
        "\n",
        "rolled_params = roll(unroll_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SupsQjxxViCU",
        "outputId": "3717ed5b-34c9-412f-e6db-37d3a5337448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "#works fine. gradient check and backprop :D\n",
        "#Had executed it before and compared the params. Did not do it again as\n",
        "#gradient check takes a looong time!\n",
        "'''\n",
        "def gradient_check(X, Y, epsilon, layers):\n",
        "    units = np.array([784, 1568, 1568, 10])\n",
        "    params = init(units)\n",
        "    num_layers = 4\n",
        "    \n",
        "    cache = forward_prop(X, Y, params, layers)\n",
        "    grads = backprop(X, Y, params, cache)\n",
        "    \n",
        "    theta = unroll(params)\n",
        "    num_theta = theta.shape[0]\n",
        "    \n",
        "    approx_grads = np.zeros((num_theta, 1))\n",
        "    \n",
        "    for i in range(num_theta):\n",
        "        print('i = {}---------------'.format(i))\n",
        "        theta_plus = np.copy(theta)\n",
        "        theta_plus[i][0] += epsilon\n",
        "        \n",
        "        params_plus = roll(theta_plus)\n",
        "        \n",
        "        cache = forward_prop(X, Y, params_plus, layers)\n",
        "        cost_plus = cost(Y, cache['A3'])\n",
        "        \n",
        "        theta_minus = np.copy(theta)\n",
        "        theta_minus[i][0] -= epsilon\n",
        "        \n",
        "        params_minus = roll(theta_minus)\n",
        "        \n",
        "        cache = forward_prop(X, Y, params_minus, layers)\n",
        "        cost_minus = cost(Y, cache['A3'])\n",
        "        \n",
        "        approx_grads[i][0] = (cost_plus - cost_minus) / (2 * epsilon)\n",
        "        \n",
        "        #print(grads[i][0])\n",
        "        print(approx_grads[i][0])\n",
        "        print('_________________________________________________________________________________')\n",
        "        \n",
        "    print(\"_____________________GRADS______________________\")\n",
        "    print(grads)\n",
        "    \n",
        "    print(\"___________________APPROXX_ +GRADS_________________\")\n",
        "    print(approx_grads)\n",
        "    \n",
        "    return grads, approx_grads\n",
        "    \n",
        "\n",
        "actual_grads, approx_grads = gradient_check(X_train, Y_train_oneHot, 0.00001, layers)\n",
        "'''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef gradient_check(X, Y, epsilon, layers):\\n    units = np.array([784, 1568, 1568, 10])\\n    params = init(units)\\n    num_layers = 4\\n    \\n    cache = forward_prop(X, Y, params, layers)\\n    grads = backprop(X, Y, params, cache)\\n    \\n    theta = unroll(params)\\n    num_theta = theta.shape[0]\\n    \\n    approx_grads = np.zeros((num_theta, 1))\\n    \\n    for i in range(num_theta):\\n        print(\\'i = {}---------------\\'.format(i))\\n        theta_plus = np.copy(theta)\\n        theta_plus[i][0] += epsilon\\n        \\n        params_plus = roll(theta_plus)\\n        \\n        cache = forward_prop(X, Y, params_plus, layers)\\n        cost_plus = cost(Y, cache[\\'A3\\'])\\n        \\n        theta_minus = np.copy(theta)\\n        theta_minus[i][0] -= epsilon\\n        \\n        params_minus = roll(theta_minus)\\n        \\n        cache = forward_prop(X, Y, params_minus, layers)\\n        cost_minus = cost(Y, cache[\\'A3\\'])\\n        \\n        approx_grads[i][0] = (cost_plus - cost_minus) / (2 * epsilon)\\n        \\n        #print(grads[i][0])\\n        print(approx_grads[i][0])\\n        print(\\'_________________________________________________________________________________\\')\\n        \\n    print(\"_____________________GRADS______________________\")\\n    print(grads)\\n    \\n    print(\"___________________APPROXX_ +GRADS_________________\")\\n    print(approx_grads)\\n    \\n    return grads, approx_grads\\n    \\n\\nactual_grads, approx_grads = gradient_check(X_train, Y_train_oneHot, 0.00001, layers)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEVGa9WvVoxb",
        "outputId": "7e54d020-8cfd-4c39-930d-f574934a3cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#curr_params = {}\n",
        "def model(X_train, Y_train, layers, iterations = 1000, learning_rate = 0.001):\n",
        "    params = init(layers)\n",
        "    num_layers = len(layers)\n",
        "    costs = []\n",
        "    \n",
        "    for i in range(iterations):\n",
        "        cache = forward_prop(X_train, Y_train, params, layers)\n",
        "        grads = backprop(X_train, Y_train, params, cache) \n",
        "        cost_iter = cost(Y_train, cache['A'+str(num_layers-1)])\n",
        "        costs.append(cost_iter)\n",
        "        print('cost = {} on iteration {}'.format(cost_iter, i))\n",
        "        params = update_params(params, grads, learning_rate)\n",
        "\n",
        "    return costs, params\n",
        "\n",
        "costs, params = model(X_train, Y_train_oneHot, layers, 400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1568, 784)\n",
            "(1568, 1)\n",
            "(1568, 1568)\n",
            "(1568, 1)\n",
            "(10, 1568)\n",
            "(10, 1)\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 5.084525826936916 on iteration 0\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 4.060808482760459 on iteration 1\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 3.5395152962135876 on iteration 2\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 3.200387521215868 on iteration 3\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 2.9432494340811886 on iteration 4\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 2.7232144241666894 on iteration 5\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 2.52961075551256 on iteration 6\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 2.3585342120842867 on iteration 7\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 2.2070391998515113 on iteration 8\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 2.0727200547897633 on iteration 9\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.9535973800829345 on iteration 10\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.8479690353824596 on iteration 11\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.7542861647232315 on iteration 12\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.6710984985646369 on iteration 13\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.5970575796626385 on iteration 14\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.5309404866739498 on iteration 15\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.471666920054845 on iteration 16\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.4183018157597032 on iteration 17\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.3700465230139165 on iteration 18\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.3262239371933366 on iteration 19\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.2862617535977872 on iteration 20\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.2496762661440157 on iteration 21\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.2160578469337366 on iteration 22\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.1850584754579332 on iteration 23\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.1563812875673987 on iteration 24\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.1297719369423416 on iteration 25\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.1050115047910023 on iteration 26\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.0819106959730065 on iteration 27\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.0603050876218378 on iteration 28\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.0400512319572919 on iteration 29\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.0210234499703408 on iteration 30\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 1.0031111836287463 on iteration 31\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.9862168002549334 on iteration 32\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.970253763946295 on iteration 33\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.9551451059488196 on iteration 34\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.9408221394647053 on iteration 35\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.9272233751384736 on iteration 36\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.9142936019958848 on iteration 37\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.9019831053753492 on iteration 38\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.8902469987687258 on iteration 39\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.8790446507743799 on iteration 40\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.8683391917929976 on iteration 41\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.8580970878477047 on iteration 42\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.8482877711264784 on iteration 43\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.8388833186374846 on iteration 44\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.8298581718235976 on iteration 45\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.8211888911689158 on iteration 46\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.8128539408011819 on iteration 47\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.8048334988917528 on iteration 48\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.7971092903126812 on iteration 49\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.7896644385549887 on iteration 50\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.7824833343646105 on iteration 51\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.7755515189295872 on iteration 52\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.76885557976756 on iteration 53\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.762383057727386 on iteration 54\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.7561223637416128 on iteration 55\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.750062704154842 on iteration 56\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.7441940136125181 on iteration 57\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.7385068946302004 on iteration 58\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.7329925630788612 on iteration 59\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.7276427989204133 on iteration 60\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.7224499016122019 on iteration 61\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.7174066496718216 on iteration 62\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.7125062639561329 on iteration 63\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.7077423742623573 on iteration 64\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.7031089889058411 on iteration 65\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6986004669696121 on iteration 66\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6942114929560937 on iteration 67\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6899370536020432 on iteration 68\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6857724166445963 on iteration 69\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6817131113497739 on iteration 70\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6777549106353752 on iteration 71\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6738938146382619 on iteration 72\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6701260355919603 on iteration 73\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6664479838945345 on iteration 74\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.662856255259095 on iteration 75\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6593476188502703 on iteration 76\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6559190063197171 on iteration 77\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6525675016623653 on iteration 78\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6492903318227972 on iteration 79\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6460848579879948 on iteration 80\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6429485675088084 on iteration 81\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6398790663979418 on iteration 82\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6368740723571495 on iteration 83\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6339314082907068 on iteration 84\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6310489962661362 on iteration 85\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6282248518867115 on iteration 86\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6254570790434232 on iteration 87\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.622743865016949 on iteration 88\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6200834759027519 on iteration 89\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6174742523347503 on iteration 90\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6149146054851136 on iteration 91\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6124030133196309 on iteration 92\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6099380170898288 on iteration 93\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6075182180445818 on iteration 94\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6051422743453632 on iteration 95\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6028088981705945 on iteration 96\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.6005168529957045 on iteration 97\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5982649510365882 on iteration 98\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5960520508451286 on iteration 99\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5938770550463293 on iteration 100\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5917389082074133 on iteration 101\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5896365948299993 on iteration 102\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5875691374571219 on iteration 103\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5855355948875015 on iteration 104\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5835350604900288 on iteration 105\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5815666606119572 on iteration 106\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5796295530747597 on iteration 107\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5777229257520682 on iteration 108\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5758459952244948 on iteration 109\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.573998005506526 on iteration 110\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5721782268410041 on iteration 111\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5703859545570397 on iteration 112\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5686205079874811 on iteration 113\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.566881229442337 on iteration 114\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5651674832347979 on iteration 115\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5634786547567221 on iteration 116\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5618141496006739 on iteration 117\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5601733927257925 on iteration 118\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5585558276649432 on iteration 119\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5569609157707852 on iteration 120\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5553881354985271 on iteration 121\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.553836981723306 on iteration 122\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5523069650902342 on iteration 123\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5507976113953096 on iteration 124\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5493084609954719 on iteration 125\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5478390682462204 on iteration 126\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5463890009652885 on iteration 127\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5449578399209725 on iteration 128\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5435451783437958 on iteration 129\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5421506214602727 on iteration 130\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5407737860476032 on iteration 131\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5394143000082097 on iteration 132\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5380718019630827 on iteration 133\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5367459408629713 on iteration 134\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5354363756165055 on iteration 135\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.534142774734389 on iteration 136\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5328648159888594 on iteration 137\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5316021860876488 on iteration 138\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5303545803617273 on iteration 139\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5291217024661502 on iteration 140\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5279032640933711 on iteration 141\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5266989846984123 on iteration 142\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5255085912353235 on iteration 143\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.524331817904388 on iteration 144\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5231684059095671 on iteration 145\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5220181032256983 on iteration 146\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5208806643749911 on iteration 147\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5197558502123885 on iteration 148\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5186434277193808 on iteration 149\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.517543169805892 on iteration 150\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5164548551198617 on iteration 151\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5153782678641778 on iteration 152\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5143131976206339 on iteration 153\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.513259439180588 on iteration 154\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5122167923820342 on iteration 155\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5111850619527984 on iteration 156\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5101640573595956 on iteration 157\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5091535926626831 on iteration 158\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5081534863758834 on iteration 159\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5071635613317308 on iteration 160\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5061836445515311 on iteration 161\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5052135671201248 on iteration 162\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5042531640651543 on iteration 163\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5033022742406462 on iteration 164\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5023607402147297 on iteration 165\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5014284081613195 on iteration 166\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.5005051277556003 on iteration 167\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4995907520731575 on iteration 168\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.49868513749260457 on iteration 169\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4977881436015678 on iteration 170\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.49689963310589147 on iteration 171\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.49601947174193706 on iteration 172\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4951475281918509 on iteration 173\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.49428367400168594 on iteration 174\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4934277835022625 on iteration 175\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4925797337326655 on iteration 176\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.49173940436626923 on iteration 177\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.49090667763920065 on iteration 178\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.49008143828113887 on iteration 179\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.48926357344836885 on iteration 180\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.48845297265899906 on iteration 181\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.48764952773026493 on iteration 182\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4868531327178349 on iteration 183\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.486063683857052 on iteration 184\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.48528107950603094 on iteration 185\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.48450522009054764 on iteration 186\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.48373600805065325 on iteration 187\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4829733477889505 on iteration 188\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.48221714562047124 on iteration 189\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4814673097240972 on iteration 190\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.48072375009547036 on iteration 191\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.47998637850133724 on iteration 192\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4792551084352767 on iteration 193\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.47852985507476514 on iteration 194\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4778105352395264 on iteration 195\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4770970673511264 on iteration 196\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.47638937139376875 on iteration 197\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.47568736887624463 on iteration 198\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.47499098279500446 on iteration 199\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4743001375983072 on iteration 200\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.47361475915141277 on iteration 201\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.47293477470278283 on iteration 202\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.47226011285125397 on iteration 203\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4715907035141515 on iteration 204\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.470926477896313 on iteration 205\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4702673684599905 on iteration 206\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.46961330889560177 on iteration 207\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4689642340933052 on iteration 208\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.46832008011536597 on iteration 209\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4676807841692944 on iteration 210\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4670462845817267 on iteration 211\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.46641652077302526 on iteration 212\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.46579143323257505 on iteration 213\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.46517096349475606 on iteration 214\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4645550541155653 on iteration 215\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4639436486498748 on iteration 216\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4633366916292974 on iteration 217\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4627341285406491 on iteration 218\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.462135905804982 on iteration 219\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4615419707571769 on iteration 220\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4609522716260723 on iteration 221\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4603667575151171 on iteration 222\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4597853783835289 on iteration 223\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4592080850279449 on iteration 224\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4586348290645452 on iteration 225\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.45806556291164113 on iteration 226\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4575002397727088 on iteration 227\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4569388136198576 on iteration 228\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4563812391777196 on iteration 229\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.45582747190774814 on iteration 230\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4552774679929101 on iteration 231\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.45473118432276666 on iteration 232\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.45418857847892513 on iteration 233\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.453649608720853 on iteration 234\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4531142339720451 on iteration 235\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.45258241380653214 on iteration 236\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4520541084357195 on iteration 237\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.45152927869555115 on iteration 238\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.45100788603398273 on iteration 239\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.45048989249876287 on iteration 240\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.44997526072550675 on iteration 241\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4494639539260572 on iteration 242\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.44895593587712757 on iteration 243\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.44845117090921105 on iteration 244\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.44794962389575815 on iteration 245\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.44745126024260634 on iteration 246\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.44695604587766313 on iteration 247\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.44646394724082644 on iteration 248\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.44597493127414556 on iteration 249\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4454889654122079 on iteration 250\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4450060175727487 on iteration 251\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4445260561474803 on iteration 252\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.44404904999312933 on iteration 253\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4435749684226812 on iteration 254\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4431037811968221 on iteration 255\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.44263545851557995 on iteration 256\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4421699710101479 on iteration 257\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4417072897348992 on iteration 258\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.44124738615957637 on iteration 259\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.440790232161656 on iteration 260\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.440335800018887 on iteration 261\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.439884062401989 on iteration 262\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.43943499236751726 on iteration 263\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4389885633508814 on iteration 264\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4385447491595181 on iteration 265\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.43810352396621377 on iteration 266\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4376648623025724 on iteration 267\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.43722873905262394 on iteration 268\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4367951294465712 on iteration 269\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.43636400905467304 on iteration 270\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4359353537812556 on iteration 271\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4355091398588546 on iteration 272\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.43508534384247954 on iteration 273\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.43466394260400115 on iteration 274\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.43424491332665727 on iteration 275\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.43382823349967414 on iteration 276\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.43341388091299926 on iteration 277\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.43300183365214684 on iteration 278\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4325920700931477 on iteration 279\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.43218456889760576 on iteration 280\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4317793090078563 on iteration 281\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.43137626964222275 on iteration 282\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.43097543029037283 on iteration 283\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.43057677070876776 on iteration 284\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4301802709162047 on iteration 285\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.42978591118945 on iteration 286\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4293936720589602 on iteration 287\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.42900353430468924 on iteration 288\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4286154789519813 on iteration 289\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.42822948726754284 on iteration 290\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4278455407554975 on iteration 291\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.42746362115351866 on iteration 292\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4270837104290382 on iteration 293\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.42670579077552917 on iteration 294\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4263298446088652 on iteration 295\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4259558545637449 on iteration 296\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.42558380349019337 on iteration 297\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.42521367445012526 on iteration 298\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4248454507139793 on iteration 299\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4244791157574147 on iteration 300\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.42411465325807235 on iteration 301\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.42375204709239916 on iteration 302\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4233912813325312 on iteration 303\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.423032340243237 on iteration 304\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.42267520827891975 on iteration 305\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4223198700806752 on iteration 306\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4219663104734052 on iteration 307\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4216145144629866 on iteration 308\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4212644672334912 on iteration 309\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.42091615414445976 on iteration 310\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.420569560728225 on iteration 311\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.42022467268728614 on iteration 312\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4198814758917296 on iteration 313\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4195399563766999 on iteration 314\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.41920010033991467 on iteration 315\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4188618941392275 on iteration 316\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4185253242902321 on iteration 317\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4181903774639141 on iteration 318\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4178570404843417 on iteration 319\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.41752530032640134 on iteration 320\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.41719514411357067 on iteration 321\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4168665591157344 on iteration 322\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.41653953274703903 on iteration 323\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.41621405256378363 on iteration 324\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4158901062623506 on iteration 325\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4155676816771723 on iteration 326\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.41524676677873273 on iteration 327\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4149273496716055 on iteration 328\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4146094185925275 on iteration 329\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.41429296190850273 on iteration 330\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.41397796811494164 on iteration 331\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4136644258338346 on iteration 332\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4133523238119528 on iteration 333\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.41304165091908346 on iteration 334\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4127323961462952 on iteration 335\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4124245486042309 on iteration 336\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4121180975214325 on iteration 337\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4118130322426952 on iteration 338\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4115093422274445 on iteration 339\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4112070170481467 on iteration 340\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.41090604638874373 on iteration 341\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4106064200431135 on iteration 342\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.41030812791355814 on iteration 343\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.41001116000931687 on iteration 344\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4097155064451033 on iteration 345\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4094211574396666 on iteration 346\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4091281033143799 on iteration 347\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4088363344918466 on iteration 348\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4085458414945349 on iteration 349\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4082566149434323 on iteration 350\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4079686455567229 on iteration 351\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.40768192414848525 on iteration 352\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4073964416274128 on iteration 353\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4071121889955558 on iteration 354\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4068291573470813 on iteration 355\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4065473378670549 on iteration 356\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.40626672183024215 on iteration 357\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4059873005999293 on iteration 358\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.40570906562676173 on iteration 359\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.40543200844760247 on iteration 360\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4051561206844088 on iteration 361\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4048813940431253 on iteration 362\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4046078203125964 on iteration 363\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.404335391363495 on iteration 364\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4040640991472683 on iteration 365\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4037939356950999 on iteration 366\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.40352489311688894 on iteration 367\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4032569636002446 on iteration 368\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4029901394094962 on iteration 369\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4027244128847197 on iteration 370\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.40245977644077824 on iteration 371\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4021962225663772 on iteration 372\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4019337438231358 on iteration 373\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.40167233284467085 on iteration 374\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.40141198233569575 on iteration 375\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4011526850711333 on iteration 376\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4008944338952412 on iteration 377\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4006372217207518 on iteration 378\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.40038104152802445 on iteration 379\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.4001258863642104 on iteration 380\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.39987174934243186 on iteration 381\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.39961862364097067 on iteration 382\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.3993665025024721 on iteration 383\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.399115379233158 on iteration 384\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.398865247202054 on iteration 385\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.3986160998402274 on iteration 386\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.3983679306400337 on iteration 387\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.39812073315438007 on iteration 388\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.3978745009959944 on iteration 389\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.397629227836707 on iteration 390\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.39738490740674415 on iteration 391\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.39714153349402936 on iteration 392\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.396899099943498 on iteration 393\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.3966576006564186 on iteration 394\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.3964170295897262 on iteration 395\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.3961773807553656 on iteration 396\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.3959386482196424 on iteration 397\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.39570082610258495 on iteration 398\n",
            "(10, 42000)\n",
            "(10, 42000)\n",
            "cost = 0.3954639085773138 on iteration 399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDzt2ZjwVz3X",
        "outputId": "c1520e44-fb30-4c49-c417-01592c434373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(params['b0'].shape[0])\n",
        "costs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1568\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.084525826936916,\n",
              " 4.060808482760459,\n",
              " 3.5395152962135876,\n",
              " 3.200387521215868,\n",
              " 2.9432494340811886,\n",
              " 2.7232144241666894,\n",
              " 2.52961075551256,\n",
              " 2.3585342120842867,\n",
              " 2.2070391998515113,\n",
              " 2.0727200547897633,\n",
              " 1.9535973800829345,\n",
              " 1.8479690353824596,\n",
              " 1.7542861647232315,\n",
              " 1.6710984985646369,\n",
              " 1.5970575796626385,\n",
              " 1.5309404866739498,\n",
              " 1.471666920054845,\n",
              " 1.4183018157597032,\n",
              " 1.3700465230139165,\n",
              " 1.3262239371933366,\n",
              " 1.2862617535977872,\n",
              " 1.2496762661440157,\n",
              " 1.2160578469337366,\n",
              " 1.1850584754579332,\n",
              " 1.1563812875673987,\n",
              " 1.1297719369423416,\n",
              " 1.1050115047910023,\n",
              " 1.0819106959730065,\n",
              " 1.0603050876218378,\n",
              " 1.0400512319572919,\n",
              " 1.0210234499703408,\n",
              " 1.0031111836287463,\n",
              " 0.9862168002549334,\n",
              " 0.970253763946295,\n",
              " 0.9551451059488196,\n",
              " 0.9408221394647053,\n",
              " 0.9272233751384736,\n",
              " 0.9142936019958848,\n",
              " 0.9019831053753492,\n",
              " 0.8902469987687258,\n",
              " 0.8790446507743799,\n",
              " 0.8683391917929976,\n",
              " 0.8580970878477047,\n",
              " 0.8482877711264784,\n",
              " 0.8388833186374846,\n",
              " 0.8298581718235976,\n",
              " 0.8211888911689158,\n",
              " 0.8128539408011819,\n",
              " 0.8048334988917528,\n",
              " 0.7971092903126812,\n",
              " 0.7896644385549887,\n",
              " 0.7824833343646105,\n",
              " 0.7755515189295872,\n",
              " 0.76885557976756,\n",
              " 0.762383057727386,\n",
              " 0.7561223637416128,\n",
              " 0.750062704154842,\n",
              " 0.7441940136125181,\n",
              " 0.7385068946302004,\n",
              " 0.7329925630788612,\n",
              " 0.7276427989204133,\n",
              " 0.7224499016122019,\n",
              " 0.7174066496718216,\n",
              " 0.7125062639561329,\n",
              " 0.7077423742623573,\n",
              " 0.7031089889058411,\n",
              " 0.6986004669696121,\n",
              " 0.6942114929560937,\n",
              " 0.6899370536020432,\n",
              " 0.6857724166445963,\n",
              " 0.6817131113497739,\n",
              " 0.6777549106353752,\n",
              " 0.6738938146382619,\n",
              " 0.6701260355919603,\n",
              " 0.6664479838945345,\n",
              " 0.662856255259095,\n",
              " 0.6593476188502703,\n",
              " 0.6559190063197171,\n",
              " 0.6525675016623653,\n",
              " 0.6492903318227972,\n",
              " 0.6460848579879948,\n",
              " 0.6429485675088084,\n",
              " 0.6398790663979418,\n",
              " 0.6368740723571495,\n",
              " 0.6339314082907068,\n",
              " 0.6310489962661362,\n",
              " 0.6282248518867115,\n",
              " 0.6254570790434232,\n",
              " 0.622743865016949,\n",
              " 0.6200834759027519,\n",
              " 0.6174742523347503,\n",
              " 0.6149146054851136,\n",
              " 0.6124030133196309,\n",
              " 0.6099380170898288,\n",
              " 0.6075182180445818,\n",
              " 0.6051422743453632,\n",
              " 0.6028088981705945,\n",
              " 0.6005168529957045,\n",
              " 0.5982649510365882,\n",
              " 0.5960520508451286,\n",
              " 0.5938770550463293,\n",
              " 0.5917389082074133,\n",
              " 0.5896365948299993,\n",
              " 0.5875691374571219,\n",
              " 0.5855355948875015,\n",
              " 0.5835350604900288,\n",
              " 0.5815666606119572,\n",
              " 0.5796295530747597,\n",
              " 0.5777229257520682,\n",
              " 0.5758459952244948,\n",
              " 0.573998005506526,\n",
              " 0.5721782268410041,\n",
              " 0.5703859545570397,\n",
              " 0.5686205079874811,\n",
              " 0.566881229442337,\n",
              " 0.5651674832347979,\n",
              " 0.5634786547567221,\n",
              " 0.5618141496006739,\n",
              " 0.5601733927257925,\n",
              " 0.5585558276649432,\n",
              " 0.5569609157707852,\n",
              " 0.5553881354985271,\n",
              " 0.553836981723306,\n",
              " 0.5523069650902342,\n",
              " 0.5507976113953096,\n",
              " 0.5493084609954719,\n",
              " 0.5478390682462204,\n",
              " 0.5463890009652885,\n",
              " 0.5449578399209725,\n",
              " 0.5435451783437958,\n",
              " 0.5421506214602727,\n",
              " 0.5407737860476032,\n",
              " 0.5394143000082097,\n",
              " 0.5380718019630827,\n",
              " 0.5367459408629713,\n",
              " 0.5354363756165055,\n",
              " 0.534142774734389,\n",
              " 0.5328648159888594,\n",
              " 0.5316021860876488,\n",
              " 0.5303545803617273,\n",
              " 0.5291217024661502,\n",
              " 0.5279032640933711,\n",
              " 0.5266989846984123,\n",
              " 0.5255085912353235,\n",
              " 0.524331817904388,\n",
              " 0.5231684059095671,\n",
              " 0.5220181032256983,\n",
              " 0.5208806643749911,\n",
              " 0.5197558502123885,\n",
              " 0.5186434277193808,\n",
              " 0.517543169805892,\n",
              " 0.5164548551198617,\n",
              " 0.5153782678641778,\n",
              " 0.5143131976206339,\n",
              " 0.513259439180588,\n",
              " 0.5122167923820342,\n",
              " 0.5111850619527984,\n",
              " 0.5101640573595956,\n",
              " 0.5091535926626831,\n",
              " 0.5081534863758834,\n",
              " 0.5071635613317308,\n",
              " 0.5061836445515311,\n",
              " 0.5052135671201248,\n",
              " 0.5042531640651543,\n",
              " 0.5033022742406462,\n",
              " 0.5023607402147297,\n",
              " 0.5014284081613195,\n",
              " 0.5005051277556003,\n",
              " 0.4995907520731575,\n",
              " 0.49868513749260457,\n",
              " 0.4977881436015678,\n",
              " 0.49689963310589147,\n",
              " 0.49601947174193706,\n",
              " 0.4951475281918509,\n",
              " 0.49428367400168594,\n",
              " 0.4934277835022625,\n",
              " 0.4925797337326655,\n",
              " 0.49173940436626923,\n",
              " 0.49090667763920065,\n",
              " 0.49008143828113887,\n",
              " 0.48926357344836885,\n",
              " 0.48845297265899906,\n",
              " 0.48764952773026493,\n",
              " 0.4868531327178349,\n",
              " 0.486063683857052,\n",
              " 0.48528107950603094,\n",
              " 0.48450522009054764,\n",
              " 0.48373600805065325,\n",
              " 0.4829733477889505,\n",
              " 0.48221714562047124,\n",
              " 0.4814673097240972,\n",
              " 0.48072375009547036,\n",
              " 0.47998637850133724,\n",
              " 0.4792551084352767,\n",
              " 0.47852985507476514,\n",
              " 0.4778105352395264,\n",
              " 0.4770970673511264,\n",
              " 0.47638937139376875,\n",
              " 0.47568736887624463,\n",
              " 0.47499098279500446,\n",
              " 0.4743001375983072,\n",
              " 0.47361475915141277,\n",
              " 0.47293477470278283,\n",
              " 0.47226011285125397,\n",
              " 0.4715907035141515,\n",
              " 0.470926477896313,\n",
              " 0.4702673684599905,\n",
              " 0.46961330889560177,\n",
              " 0.4689642340933052,\n",
              " 0.46832008011536597,\n",
              " 0.4676807841692944,\n",
              " 0.4670462845817267,\n",
              " 0.46641652077302526,\n",
              " 0.46579143323257505,\n",
              " 0.46517096349475606,\n",
              " 0.4645550541155653,\n",
              " 0.4639436486498748,\n",
              " 0.4633366916292974,\n",
              " 0.4627341285406491,\n",
              " 0.462135905804982,\n",
              " 0.4615419707571769,\n",
              " 0.4609522716260723,\n",
              " 0.4603667575151171,\n",
              " 0.4597853783835289,\n",
              " 0.4592080850279449,\n",
              " 0.4586348290645452,\n",
              " 0.45806556291164113,\n",
              " 0.4575002397727088,\n",
              " 0.4569388136198576,\n",
              " 0.4563812391777196,\n",
              " 0.45582747190774814,\n",
              " 0.4552774679929101,\n",
              " 0.45473118432276666,\n",
              " 0.45418857847892513,\n",
              " 0.453649608720853,\n",
              " 0.4531142339720451,\n",
              " 0.45258241380653214,\n",
              " 0.4520541084357195,\n",
              " 0.45152927869555115,\n",
              " 0.45100788603398273,\n",
              " 0.45048989249876287,\n",
              " 0.44997526072550675,\n",
              " 0.4494639539260572,\n",
              " 0.44895593587712757,\n",
              " 0.44845117090921105,\n",
              " 0.44794962389575815,\n",
              " 0.44745126024260634,\n",
              " 0.44695604587766313,\n",
              " 0.44646394724082644,\n",
              " 0.44597493127414556,\n",
              " 0.4454889654122079,\n",
              " 0.4450060175727487,\n",
              " 0.4445260561474803,\n",
              " 0.44404904999312933,\n",
              " 0.4435749684226812,\n",
              " 0.4431037811968221,\n",
              " 0.44263545851557995,\n",
              " 0.4421699710101479,\n",
              " 0.4417072897348992,\n",
              " 0.44124738615957637,\n",
              " 0.440790232161656,\n",
              " 0.440335800018887,\n",
              " 0.439884062401989,\n",
              " 0.43943499236751726,\n",
              " 0.4389885633508814,\n",
              " 0.4385447491595181,\n",
              " 0.43810352396621377,\n",
              " 0.4376648623025724,\n",
              " 0.43722873905262394,\n",
              " 0.4367951294465712,\n",
              " 0.43636400905467304,\n",
              " 0.4359353537812556,\n",
              " 0.4355091398588546,\n",
              " 0.43508534384247954,\n",
              " 0.43466394260400115,\n",
              " 0.43424491332665727,\n",
              " 0.43382823349967414,\n",
              " 0.43341388091299926,\n",
              " 0.43300183365214684,\n",
              " 0.4325920700931477,\n",
              " 0.43218456889760576,\n",
              " 0.4317793090078563,\n",
              " 0.43137626964222275,\n",
              " 0.43097543029037283,\n",
              " 0.43057677070876776,\n",
              " 0.4301802709162047,\n",
              " 0.42978591118945,\n",
              " 0.4293936720589602,\n",
              " 0.42900353430468924,\n",
              " 0.4286154789519813,\n",
              " 0.42822948726754284,\n",
              " 0.4278455407554975,\n",
              " 0.42746362115351866,\n",
              " 0.4270837104290382,\n",
              " 0.42670579077552917,\n",
              " 0.4263298446088652,\n",
              " 0.4259558545637449,\n",
              " 0.42558380349019337,\n",
              " 0.42521367445012526,\n",
              " 0.4248454507139793,\n",
              " 0.4244791157574147,\n",
              " 0.42411465325807235,\n",
              " 0.42375204709239916,\n",
              " 0.4233912813325312,\n",
              " 0.423032340243237,\n",
              " 0.42267520827891975,\n",
              " 0.4223198700806752,\n",
              " 0.4219663104734052,\n",
              " 0.4216145144629866,\n",
              " 0.4212644672334912,\n",
              " 0.42091615414445976,\n",
              " 0.420569560728225,\n",
              " 0.42022467268728614,\n",
              " 0.4198814758917296,\n",
              " 0.4195399563766999,\n",
              " 0.41920010033991467,\n",
              " 0.4188618941392275,\n",
              " 0.4185253242902321,\n",
              " 0.4181903774639141,\n",
              " 0.4178570404843417,\n",
              " 0.41752530032640134,\n",
              " 0.41719514411357067,\n",
              " 0.4168665591157344,\n",
              " 0.41653953274703903,\n",
              " 0.41621405256378363,\n",
              " 0.4158901062623506,\n",
              " 0.4155676816771723,\n",
              " 0.41524676677873273,\n",
              " 0.4149273496716055,\n",
              " 0.4146094185925275,\n",
              " 0.41429296190850273,\n",
              " 0.41397796811494164,\n",
              " 0.4136644258338346,\n",
              " 0.4133523238119528,\n",
              " 0.41304165091908346,\n",
              " 0.4127323961462952,\n",
              " 0.4124245486042309,\n",
              " 0.4121180975214325,\n",
              " 0.4118130322426952,\n",
              " 0.4115093422274445,\n",
              " 0.4112070170481467,\n",
              " 0.41090604638874373,\n",
              " 0.4106064200431135,\n",
              " 0.41030812791355814,\n",
              " 0.41001116000931687,\n",
              " 0.4097155064451033,\n",
              " 0.4094211574396666,\n",
              " 0.4091281033143799,\n",
              " 0.4088363344918466,\n",
              " 0.4085458414945349,\n",
              " 0.4082566149434323,\n",
              " 0.4079686455567229,\n",
              " 0.40768192414848525,\n",
              " 0.4073964416274128,\n",
              " 0.4071121889955558,\n",
              " 0.4068291573470813,\n",
              " 0.4065473378670549,\n",
              " 0.40626672183024215,\n",
              " 0.4059873005999293,\n",
              " 0.40570906562676173,\n",
              " 0.40543200844760247,\n",
              " 0.4051561206844088,\n",
              " 0.4048813940431253,\n",
              " 0.4046078203125964,\n",
              " 0.404335391363495,\n",
              " 0.4040640991472683,\n",
              " 0.4037939356950999,\n",
              " 0.40352489311688894,\n",
              " 0.4032569636002446,\n",
              " 0.4029901394094962,\n",
              " 0.4027244128847197,\n",
              " 0.40245977644077824,\n",
              " 0.4021962225663772,\n",
              " 0.4019337438231358,\n",
              " 0.40167233284467085,\n",
              " 0.40141198233569575,\n",
              " 0.4011526850711333,\n",
              " 0.4008944338952412,\n",
              " 0.4006372217207518,\n",
              " 0.40038104152802445,\n",
              " 0.4001258863642104,\n",
              " 0.39987174934243186,\n",
              " 0.39961862364097067,\n",
              " 0.3993665025024721,\n",
              " 0.399115379233158,\n",
              " 0.398865247202054,\n",
              " 0.3986160998402274,\n",
              " 0.3983679306400337,\n",
              " 0.39812073315438007,\n",
              " 0.3978745009959944,\n",
              " 0.397629227836707,\n",
              " 0.39738490740674415,\n",
              " 0.39714153349402936,\n",
              " 0.396899099943498,\n",
              " 0.3966576006564186,\n",
              " 0.3964170295897262,\n",
              " 0.3961773807553656,\n",
              " 0.3959386482196424,\n",
              " 0.39570082610258495,\n",
              " 0.3954639085773138]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAjGm3NjcrGb"
      },
      "source": [
        "def predict(params, X, Y, layers):\n",
        "    cache = forward_prop(X, Y, params, layers)\n",
        "    \n",
        "    maxx = np.max(cache['A3'], axis = 0)\n",
        "    one_hot_pred = np.where(maxx == cache['A3'], 1, 0)\n",
        "    pred = [np.where(r==1)[0][0] for r in one_hot_pred.T]\n",
        "    return np.array(pred).reshape(1, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEcUeysIdV0c"
      },
      "source": [
        "'''\n",
        "Accuracies:\n",
        "75%: 500 epochs, 0.0001 lr and weights init to reduce exploding/ vanishing gradients. dev set - 75.5% test set - 77.33%\n",
        "88%: 400 epochs, 0.001 lr and same weights as above; dev set - 87.8%, test set - 89.4%\n",
        "'''\n",
        "\n",
        "train_pred = predict(params, X_train, Y_train, layers)   #Not sending Y_train_oneHot here because we are using y with target numbers for caclulating accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL-AQR2odydA",
        "outputId": "d9c3ace3-8c1f-48e1-b41e-6668ec863d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def accuracy(Y, Y_pred):\n",
        "    return np.sum(Y_pred == Y)/ Y.shape[1] * 100\n",
        "\n",
        "train_accu = accuracy(Y_train, train_pred)\n",
        "\n",
        "print(train_accu)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88.25952380952381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "todOq3IFernp"
      },
      "source": [
        "dev_pred = predict(params, X_dev, Y_dev, layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwa3Xe5xesod",
        "outputId": "ee54e040-d887-44d8-fe86-69960714c579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "dev_accu = accuracy(Y_dev, dev_pred)\n",
        "\n",
        "print(dev_accu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "87.82857142857144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-XvpGYve4x_"
      },
      "source": [
        "test_pred = predict(params, X_test, Y_test, layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1_5Tqnwe-r4",
        "outputId": "0dfa5250-0b5e-4978-e233-101b272d77d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_accu = accuracy(Y_test, test_pred)\n",
        "\n",
        "print(test_accu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "89.4857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}